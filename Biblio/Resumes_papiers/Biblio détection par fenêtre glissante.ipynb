{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblio sur la façon de faire passer une fenêtre glissante sur une grande image pour détecter un objet #\n",
    "\n",
    "Ici, je vais regrouper l'ensemble des documents qui donnent des informations sur la détection d'objets.\n",
    "\n",
    "## Cours de Jen-Philippe Tarel sur l'introduction d'objets dans les images ##\n",
    "\n",
    "Dans ce cours, les différentes méthodes pour la détection d'un objet dans les images sont présentées. On retrouve parmi elles des méthodes basique de traîtement d'image (corrélation, détection de contours) jusqu'à des méthodes d'apprentissage sur image plus complexes (ACP, SVD, SVM).\n",
    "\n",
    "Lien du cours : http://elynxsdk.free.fr/ext-docs/Demosaicing/more/news1/fvo_jpt.pdf\n",
    "\n",
    "## Papier sur la détection d'objets en utilisant un CNN avec scale dependant pooling et le cascaded rejection classifiers ##\n",
    "\n",
    "Le scale-dependant pooling augmente la précision de la détection surtout sur les petits objets tandis que le cascaded rejecction classifiers augmente la vitesse de détection des objets. \n",
    "\n",
    "Scale dependant pooling : on prend en sortie les box qui ont été proposées par le réseau Fast RCNN. Chacune de ces régions est divisée en une grille de taille 7x7 ou 6x6 et les caractéristiques sont regroupées en utilisant un max pooling sur chacune des grilles. La méthode SDP examine la hauteur de chacune des box proposées et regroupes ses caractéristiques avec une couche de convolution en fonction de la hauteur de la box.\n",
    "\n",
    "Cascaded Rejection Classifiers : L'objectif est de réduire le temps de calcul afin de réduire rapidement le nombre de propositions de régions afin de ne garder que les régions qui ont de grandes chances de contenir un objet pour effectuer sur elles les opérations de calcul complexes et chronophages. Contrairement aux méthodes classiques, il n'y a pas d'autres couches qui sont rajoutées eu réseau afin de pré-traiter les données mais les régions négatives sont éliminées au fur et à mesure des convolutions. \n",
    "\n",
    "Lien de l'article : https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf\n",
    "\n",
    "## Papier sur le réseau HyperNet ##\n",
    "\n",
    "Problème de l'état de l'art : détection des objets de petite taille avec une localisation précise. \n",
    "\n",
    "Architecture du réseau : \n",
    "- Prend une image en entrée ;\n",
    "- Calcule la représentation en Hyper Feature (informations grossières combinées à des informations précises) ;\n",
    "- Génère 100 propositions de régions avec des bounding boxes ;\n",
    "- Classifie et ajuste chacune des régions.\n",
    "\n",
    "Lien de l'article : https://arxiv.org/pdf/1604.00600.pdf\n",
    "\n",
    "## Fast R-CNN ##\n",
    "\n",
    "Combinaison des réseaux de neurones convolutifs et de Selective Search pour faire de la reconnaissance d'objets. Ce réseau a été mis en place car les SVMs utilisés précédemment pour la reconnaissance d'objets à la fin du selective search étaient longs à entraîner. \n",
    "\n",
    "Architecture du réseau : \n",
    "- En entrée on prend l'image et les propositions d'objets (appelées régions d'intérêt, RoI) ;\n",
    "- Toutes ces informations sont envoyées à un réseau de neurones convolutif ;\n",
    "- chaque RoI est mise en commun avec une feature map de taille fixe et ensuite cartographié dans un vecteur de caratéristiques avec une couche fully connected ;\n",
    "- Le réseau a deux sorties par RoI : les proba ressorties avec une fonction softmax et et la regression de la correction des bounding boxes. \n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1504.08083.pdf\n",
    "\n",
    "Lien autre article : http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_A-Fast-RCNN_Hard_Positive_CVPR_2017_paper.pdf\n",
    "\n",
    "## Faster R-CNN ##\n",
    "\n",
    "Composé de deux modules : le premier est un réseau de neurones convolutif profond qui propose des régions et le deuxième est un détecteur Fast R-CNN qui utilise les régions proposées afin de procéder à la classification et à la régression des bounding boxes.\n",
    "\n",
    "## Mask R-CNN ##\n",
    "\n",
    "Problèmes avec les Fast R-CNN :\n",
    "- l'apprentissage est couteux en espace et en temps\n",
    "- La détection d'objet est lente\n",
    "\n",
    "L'objectif est de savoir si chacun des pixels appartient à l'objet au lieu de proposer une bounding box qui le contient. \n",
    "\n",
    "Architecture du réseau :\n",
    "- En entrée prend une image ;\n",
    "- Réalise la CNN feature map de cette image ;\n",
    "- Les couches de convolution vont analyser l'image pixel par pixel et en sortie on obtient une matrice de la taille de la région proposée qui contient 1 si le pixel appartient à l'objet et 0 sinon. \n",
    "\n",
    "C'est une extension du réseau faster R-CNN qui ajoute une branche pour prédire le masque de l'objet en parallèle avec la prédiction de la bounding box. \n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1703.06870.pdf%20http://arxiv.org/abs/1703.06870.pdf\n",
    "https://medium.com/@umerfarooq_26378/from-r-cnn-to-mask-r-cnn-d6367b196cfd\n",
    "\n",
    "## RetinaNet ##\n",
    "\n",
    "Réseaux de détections classiques fonctionnent en deux étapes : la première étape consiste à proposer des régions et la deuxième étape à évaluer ces régions pour ne retenir que les plus pertinentes. RetinaNet est composé d'un backbone network (qui est un Feature Pyramid Network, généralement ResNet50 ou ResNet101) et de deux task-specific network (un qui sert à classifier objet/non objet les sorties du backbone et le deuxième qui fait de la régression sur les bounding boxes retenues). \n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1708.02002.pdf\n",
    "\n",
    "## Feature Pyramid Network ##\n",
    "\n",
    "Cet architecture a été proposée afin de rendre plus rapide les R-CNN. \n",
    "Dans les RPN classiques, la segmentation se fait sur la carte des caractéristiques de convolution finale et le principe du Pyramidal Network est de faire cette segmentation sur toutes les cartes des caractéristiques de convolution. En effet, les cartes de caractéristiques de convolution intermédiaire ayant des tailles différentes, ceci s'apparente à une architecture pyramidale. Ajout du focal loss qui permet d'ajouter beaucoup de poids aux exemples mal classifiés et peu de poids aux exemples bien classifiés, le focal loss de chacun des anchors est calculé. \n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1612.03144.pdf\n",
    "\n",
    "## SSD : Single Shot MultiBox Detector ##\n",
    "\n",
    "Explication du nom de cette architecture :\n",
    "- Single Shot : la localisation et la classification on été faites en un seul passage du réseau ;\n",
    "- MultiBox : le nom de la technique de la regression des bounding boxes ;\n",
    "- Detector : détecte les objets et les classifie.\n",
    "\n",
    "Architecture :\n",
    "- L'image est passée en entrée d'un réseau basé sur l'architecture de VGG16 (bonne performance sur les images de haute qualité) à laquelle on a enlevé les couches fully connected pour y mettre des couches de convolution ;\n",
    "- Ensuite, il y a une regression de bounding box à l'aide de la méthode MultiBox (réseau de neurones convolutif)\n",
    "\n",
    "Algorithme rapide mais qui est aussi précis que les algorithmes de détection tels que Faster R-CNN. L'amélioration de la vitesse vient surtout de l'élimination des propositions de bounding boxes. \n",
    "\n",
    "En résumé : SSD est plus rapide que les single shot detectors de l'état de l'art (YOLO) et plus précis mais aussi précis que les techniques de détection plus lentes telles que Faster R-CNN. \n",
    "\n",
    "Lien de l'article : https://arxiv.org/pdf/1512.02325.pdf\n",
    "https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab\n",
    "\n",
    "## YOLO ##\n",
    "\n",
    "Comparé à l'état de l'art, le réseau YOLO fait plus d'erreurs de localisation mais est moins enclin à prédire des faux positifs dans l'arrière plan. Réseau extrêmement rapide (45 frames traitées par seconde).\n",
    "\n",
    "Fonctionnement de l'algorithme :\n",
    "- Redimensionnement de l'image en taille 448x448 ;\n",
    "- Traitement de l'image redimensionnée avec un réseau de neurones convolutif ;\n",
    "- Seuillage des résultats de détection.\n",
    "\n",
    "YOLO, contraîrement à la technique de la fenêtre glissante ou des méthodes de proposition de région voit l'image en entier durant la phase d'apprentissage et de test, ce qui lui permet d'associer des informations contextuelles aux classes prédites. \n",
    "\n",
    "La précision de détection (surtout pour les petits objets) est en-dessous de l'état de l'art mais la vitesse de détection est bien supérieure aux systèmes déjà existants.\n",
    "\n",
    "Le système divise une image en une grille de taille SxS. Si le centre d'un objet est dans une cellule de la grillen cette cellule est responsable de la détection de cet objet. Chacune de cellules de la grille prédit B bouding boxes et un score de confiance pour chacune de ces bounding boxes. Ces scores indiquent la probabilité que la box contienne un objet et si elle est à la bonne taille (Pr(Objet) + IoU). Enfin, pour chacune des cellules, une proba d'appartenance à une classe en admettant qu'elle contienne un objet est calculée. En conbinant cette carte de calcul de proba aux bouding boxes prédites, on obtient les détections finales. \n",
    "\n",
    "Lien article : https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n",
    "\n",
    "\n",
    "## Selective search ##\n",
    "\n",
    "3 façons de reconnaître des régions : \n",
    "- La recherche exhaustive qui a pour but de trouver une bounding box pour chacun des objets d'une image pour cela on utilise des fenêtres de différentes échelles et les font glisser sur l'ensemble de l'image ;\n",
    "- La segmentation renvoie chaque pixel à une classe donnée et les pixels appartenant à la même classe constituent un objet ;\n",
    "- La recherche sélective est un mélange de la recherche exhaustive et de la segmentation en segmentant l'image au préalable afin de définir des régions sur lesquelles il faut se concentrer pour prédire des box. Elle augmente le temps de détection des objets et permet de faire de la détection d'objet en temps réel. Elle est utilisé dans les Fast-RCNN par exemple.\n",
    "\n",
    "Fonctionnement de l'algorithme :\n",
    "- On initialise des box ;\n",
    "- Les box sont fusionnées avec le voisin duquel elles sont le plus similaires jusqu'à obtenir une box contenant toute l'image;\n",
    "- Lorsque l'on conserve toutes les box que l'on a créé, on peut détecter des objets de toutes les échelles ;\n",
    "- Afin d'éviter de privilégier les box les plus grandes, l'index des box (1 pour la plus récente) est multiplié par un nombre aléatoire entre 0 et 1. \n",
    "\n",
    "Afin d'améliorer la robustesse de la recherche, les auteurs utilisent trois stratégies différentes :\n",
    "- Recherche dans différents espaces de couleur (RBG, Echelle de gris, Lab (une valeur pour la luminosité, une pour le vert-rouge et une pour je jaune-bleu), HSV, etc. ;\n",
    "- Différentes initialisations de la taille des régions de départ ;\n",
    "- Différentes mesures de similarité (mesure de la similarité de couleur, mesure de la similarité de texture, la similarité de taille des box (éviter le déséquilibre entre les tailles des box finales), la caractéristique de fitness (permet de fusionner 2 box qui se chevauchent assez tôt dans l'algorithme. \n",
    "\n",
    "Une fois les bounding boxes trouvées, elles sont combinées à des SVMs afin qu'elles puissent avoir une application en reconnaissance d'image.\n",
    "\n",
    "Lien article : http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\n",
    "\n",
    "## PointNet ##\n",
    "\n",
    "Classification et segmentation d'un nuage de points 3D. \n",
    "\n",
    "Problématique : données 3D converties dans un format qui les rend trop volumineuses. \n",
    "\n",
    "Réseaux existants pour traiter données 3D :\n",
    "- Volumetric CNNs : pionier dans le traîtemant d'information 3D, mais contrainte de résolution de l'objet (FPNN et Vote3D) et ne peuvent pas traiter de gros nuages de points ;\n",
    "- Les Multiview CNNs ont transformé des données 3D en données 2D afin de pouvoir les traiter mais il y a une perte d'information donc ne peuvent être utilisé pour la classification 3D et la reconstruction de silhouette 3D ;\n",
    "- Spectral CNNs : peu efficaces sur les objets tels que les meubles ;\n",
    "- Feature based DNNs : Les caractéristiques 3D sont présentées sous forme de vecteur, il y a donc une perte d'information.\n",
    "\n",
    "Donnée 3D représentée par ses coordonnées (x,y,z). \n",
    "\n",
    "Architecture du réseau :\n",
    "- n points passés en entrée ;\n",
    "- application de l'input transformation et de la feature transformation ;\n",
    "- couche de maxpooling pour regrouper les caractéristiques de l'objet ;\n",
    "- sortie des scores de classification pour k classes ;\n",
    "- multi-layer-perceptron pour la segmentation qui est une extension de la classification.\n",
    "\n",
    "\n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1612.00593.pdf\n",
    "\n",
    "## Frustum PointNet ##\n",
    "\n",
    "Problème de l'état de l'art : beaucoup de réseaux pour la détection d'objets 2D (masque et bounding box) mais peu pour la détection d'objet 3D. Frustum PointNet va servir à la détection d'objets 3D. \n",
    "\n",
    "Capable de classifier un nuage de points 3D ou encore d'associer chaque point d'un nuage de points 3D à une classe. \n",
    "\n",
    "Fonctionnement : \n",
    "- Extraction du tronc de cône contenant l'objet 3D (frustum) ;\n",
    "- Détection du masque 3D de l'objet à l'aide d'un réseau de segmentation ;\n",
    "- Création de la bounding box 3D de l'objet à l'aide d'un réseau de régression ;\n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1711.08488.pdf\n",
    "\n",
    "## NASNet ##\n",
    "\n",
    "I.A créée par Google à partir d'AutoML (IA qui a pour but de générer d'autres IA).\n",
    "\n",
    "Objectif : Pouvoir reconnaître en temps réel par le biais de la vidéo beaucoup d'objets (voitures, personnes, signalisation routière, sac à dos, etc.) Ses performances sont améliorées par AutoML et tente ensuite de les améliorer. Aujourd'hui a surpassé n'importe quelle autre IA en matière de détection au sein d'une image. \n",
    "\n",
    "Lien article : https://arxiv.org/pdf/1707.07012.pdf\n",
    "https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html\n",
    "\n",
    "## DSSD ##\n",
    "\n",
    "Plus précis que SSD tout en maintenant la même vitesse de détection.\n",
    "\n",
    "Lien vers l'article : https://arxiv.org/pdf/1701.06659.pdf\n",
    "\n",
    "## BD pour la détection d'objets ##\n",
    "\n",
    "### CIFAR 10 & CIFAR 100 ###\n",
    "\n",
    "CIFAR 10 : 60 000 images couleur (50 000 train et 10 000 test) 32x32 appartenant à 10 classes.\n",
    "\n",
    "CIFAR 100 : 60 000 images couleur 32x32 appartenant à 100 classes et regroupées en 20 super classes. Chaque classe contient 600 images. \n",
    "\n",
    "Bases utilisées pour la classification d'objet.\n",
    "\n",
    "Informations sur les bases : https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "### COCO (Comon Object in COntext) ###\n",
    "\n",
    "Plus de 120 000 images multilabels contenant plus de 880 000 objets étiquetés en 91 classes et une classe non étiquetée (afin de regrouper le reste de l'image non étiqueté). Utilisée pour la détection, la segmentation et la classification d'objets.\n",
    "\n",
    "Informations sur la base : http://cocodataset.org/#explore\n",
    "\n",
    "### Pascal VOC (Visual Object Classes) ###\n",
    "\n",
    "\n",
    "\n",
    "Informations sur la base : http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "\n",
    "### KITTI ##\n",
    "\n",
    "BD sur scènes routières utilisant des images RBG, NB un GPS et un Lidar. Pour plus d'info voir Le notebook jupyter qui correspond.\n",
    "\n",
    "Lien vers site de la BD : http://www.cvlibs.net/datasets/kitti/\n",
    "\n",
    "### CEREMA AWP ###\n",
    "\n",
    "Détection de piétons en conditions météorologiques dégradées. 3 types de conditions : brouillard, pluie ou dégagé de jour comme de nuit. Les images ont été simulées dans un tunnel capable de reproduire ces conditions météorologiques artificiellement. \n",
    "\n",
    "Lien vers site de la BD : \n",
    "\n",
    "### UA Detrac ###\n",
    "\n",
    "Détection de véhicules dans différentes conditions météorologiques et d'éclaircissement (nuageux, dégagé, pluie et nuit). \n",
    "\n",
    "Lien vers site de la BD : https://detrac-db.rit.albany.edu\n",
    "\n",
    "## Vocabulaire ##\n",
    "\n",
    "RoI : Region of Interest\n",
    "IoU : Intersection over Union (area of overlap / area of union)\n",
    "Voxel : Volumetric Pixel, pixel en 3D utilisé pour la représentation 3D dans l'espace (utilisé surtout en imagerie médicale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
